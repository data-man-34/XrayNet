{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "imgdir = r'C:\\Science Research\\xraynet_data\\images\\images'\n",
    "h5path = r'C:\\Science Research\\xraynet_data\\images\\images\\dataset.h5'\n",
    "batch_size = 256\n",
    "num_classes = 2\n",
    "epochs = 24\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 128, 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('dataset.h5'):\n",
    "    from tflearn.data_utils import build_hdf5_image_dataset #module to create data and labels from given images\n",
    "\n",
    "    #Deletes hidden .DS_Store file from sub\n",
    "    try:\n",
    "        for sub in os.listdir(imgdir):\n",
    "            if not sub  == '.DS_Store':\n",
    "                os.remove('{}/{}/.DS_Store'.format(imgdir,sub))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    build_hdf5_image_dataset(imgdir, #create the dataset from the images in imgdir\n",
    "                             image_shape=(128, 128), #resizes all the images to a uniform size for training\n",
    "                             mode='folder', #the images are in a folder\n",
    "                             output_path=h5path, #saves data to a file\n",
    "                             categorical_labels=True, #create labels for each category, normal or abnormal\n",
    "                             grayscale=True, #make images black and white to remove unnecissary color noise\n",
    "                             files_extension=['.png'],\n",
    "                             normalize=True) #normalize the images by dividing by 255\n",
    "\n",
    "    ##Only need run the above once##\n",
    "else:\n",
    "    print('H5 already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LOAD IMAGES##\n",
    "import h5py\n",
    "import numpy as np #library for scientific computing\n",
    "from tflearn.data_utils import shuffle\n",
    "h5f = h5py.File(h5path, 'r') #open the file 'dataset.h5' in reading mode (like open())\n",
    "data = h5f['X'][:-8704] # set the data to X\n",
    "labels = h5f['Y'][:-8704] #set the labels to Y\n",
    "data, labels = shuffle(data,labels) #shuffles the data and labels in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 51708, 1.0: 51708})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(map(lambda x: x[0],labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [0,1] == Normal\n",
    "\n",
    "# [1,0] == Abnormal\n",
    "\n",
    "# [Abnormalness, Normalness]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##CONSTRUCTING THE NETWORK##\n",
    "\n",
    "#Below: import all of the layers and libraries for a TFLearn CNN\n",
    "from tflearn.layers.core import input_data, fully_connected, dropout\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "net = input_data(shape=(None,128,128)) #create layer to input data, in the same shape as our images (128x128)\n",
    "net = conv_2d(net, 32, 3, activation='relu',regularizer='L2') # Create first convolutional layer, with 32 neurons, and \n",
    "#rectfied linear activation function\n",
    "net = max_pool_2d(net, 2)\n",
    "#create first max pool layer, which gets the maximum value from the area created by the filter\n",
    "\n",
    "net = conv_2d(net, 64, 3, activation='relu',regularizer='L2') #same as above, but w/ 64 neurons\n",
    "net = max_pool_2d(net, 2)\n",
    "\n",
    "net = conv_2d(net, 128, 3, activation='relu',regularizer='L2') #same as above, but w/ 128 neurons\n",
    "net = max_pool_2d(net, 2)\n",
    "\n",
    "net = conv_2d(net, 256, 3, activation='relu',regularizer='L2') #256 neurons\n",
    "net = max_pool_2d(net, 2)\n",
    "\n",
    "net = conv_2d(net, 512, 3, activation='relu',regularizer='L2') #512 neurons\n",
    "net = max_pool_2d(net, 2)\n",
    "\n",
    "net = fully_connected(net, 1024, activation='relu') #1024 neurons\n",
    "net = dropout(net, 0.8) #keep 80% of the outputs from this layer, given by .8 parameter\n",
    "#helps eliminate chance for overfitting (the neural network memorizing the dataset)\n",
    "\n",
    "net = fully_connected(net, 2, activation='softmax') #Create fully connected layer, \n",
    "#which is representative of our outputs, either normal, or abnormal\n",
    "#uses softmax activation function, which is commonly used as last layer of NN\n",
    "\n",
    "net = regression(net, optimizer='adam', learning_rate=1e-4, #perform logistic regression on the output, to try and create a model\n",
    "#that matches the dataset, even though we have more than one output (why we can't perform linear regression)\n",
    "                     loss='categorical_crossentropy') #use the categorical crossentropy loss function, used for binary classficiation (2 outputs - normal or abnormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##SETUP THE TRAINING##\n",
    "\n",
    "from tflearn.models import dnn #import library for deep neural network\n",
    "model = dnn.DNN(net, tensorboard_dir='log',tensorboard_verbose=3) #create the model from the net\n",
    "#establish the directory to store the logs of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##TRAIN##\n",
    "model.fit(data, labels, n_epoch=24, #train the model on the data and corresponding labels\n",
    "           validation_set=.1, #establish set to validate the model on (aka the last 10% of the dataset)\n",
    "           snapshot_step=200, show_metric=True, run_id='italiaNet') #show the progress of the NN, and give it a name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
