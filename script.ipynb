{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7f4c4917-22fb-4b10-879e-51c600e6c3af",
    "_uuid": "9b086e5ec535ad75eada3ca72bf5e6534251074f"
   },
   "source": [
    "# Overview\n",
    "\n",
    "The new model CapsuleNet proposed by Sara Sabour (and Geoffry Hinton) claims to deliver state of the art results on [MNIST](https://arxiv.org/abs/1710.09829). The kernel aims to create and train the model using the Kaggle Dataset and then make a submission to see where it actually ends up. Given the constraint of using a Kaggle Kernel means it can't be trained as long as we would like or with GPU's but IMHO if a model can't be reasonably well trained in an hour on a 28x28 dataset, that model probably won't be too useful in the immediate future.\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "* Keras implementation of CapsNet in Hinton's paper Dynamic Routing Between Capsules.\n",
    "* Code adapted from https://github.com/XifengGuo/CapsNet-Keras/blob/master/capsulenet.py\n",
    "*  Author: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n",
    "*     The current version maybe only works for TensorFlow backend. Actually it will be straightforward to re-write to TF code.\n",
    "*     Adopting to other backends should be easy, but I have not tested this. \n",
    "\n",
    "Result:\n",
    "    Validation accuracy > 99.5% after 20 epochs. Still under-fitting.\n",
    "    About 110 seconds per epoch on a single GTX1070 GPU card\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "e30e2e10-a909-485d-be9d-dc6f592911a7",
    "_uuid": "c7e569699c6d067cd9fdf9c77299775e399b2ef3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from tqdm import tqdm_notebook\n",
    "import cv2\n",
    "\n",
    "IMG_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7d75c4e0-ffca-45fc-8acc-620fe2825f82",
    "_uuid": "3e810fabad89045b7f4b51fe8540164f90e7698c"
   },
   "source": [
    "# Load Data\n",
    "Here we load and reformat the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "9dbcd67c-8f99-4d8b-8cf7-480d8dc069a4",
    "_uuid": "526436cc40013621251285812ba95725d4a6d749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from file...\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "os.chdir('C:/Science_Research/')\n",
    "print('loading from file...')\n",
    "X_val = np.load(f'image_batches/X_val.npy')\n",
    "Y_val = np.load(f'image_batches/Y_val.npy')\n",
    "X_train = np.load(f'image_batches/X_train_{0}.npy')[:5000]\n",
    "Y_train = np.load(f'image_batches/Y_train_{0}.npy')[:5000]\n",
    "print('Data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "d6e78705-f643-40c7-9f1a-d0564d64c718",
    "_uuid": "642af6625a73b7e67193b8d96d23c22d30073643"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa3b35008624352a367029acd8da30c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a1383e523d41b787292317011e658d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BW_train = []\n",
    "BW_val = []\n",
    "\n",
    "for image in tqdm_notebook(X_train):\n",
    "    im = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    BW_train.append(cv2.resize(im, (IMG_SIZE,IMG_SIZE)))\n",
    "   \n",
    "for image in tqdm_notebook(X_val):\n",
    "    im = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    BW_val.append(cv2.resize(im, (IMG_SIZE,IMG_SIZE)))\n",
    "   \n",
    "X_train = np.array(BW_train)\n",
    "X_val = np.array(BW_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "Y_train = 1 - Y_train.astype('int')\n",
    "Y_train = np.eye(2)[Y_train]\n",
    "\n",
    "Y_val = 1 - Y_val.astype('int')\n",
    "Y_val = np.eye(2)[Y_val]\n",
    "\n",
    "Y_train = Y_train.reshape(-1,2)\n",
    "Y_val = Y_val.reshape(-1,2)\n",
    "\n",
    "print(Y_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = X_train.reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "y_train = Y_train\n",
    "\n",
    "x_test = X_val.reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "y_test = Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9776fd57-44e0-4211-a7a5-c7e647a10704",
    "_uuid": "f4b5499a472b312d5c5f0274ad429567aced6841"
   },
   "source": [
    "# Capsule Layers \n",
    "Here is the implementation of the necessary layers for the CapsuleNet. These are not optimized yet and can be made significantly more performant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "167d39ca-ee32-4eec-a83b-86194252b14f",
    "_uuid": "90c180a9a8c20e3fb8a93c3eb42588927cfcd6b6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras import initializers, layers\n",
    "\n",
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n",
    "    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n",
    "    output: shape=[dim_1, ..., dim_{n-1}]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]\n",
    "\n",
    "class Mask(layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1.\n",
    "    Output shape: [None, d2]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # use true label to select target capsule, shape=[batch_size, num_capsule]\n",
    "        if type(inputs) is list:  # true label is provided with shape = [batch_size, n_classes], i.e. one-hot code.\n",
    "            assert len(inputs) == 2\n",
    "            inputs, mask = inputs\n",
    "        else:  # if no true label, mask by the max length of vectors of capsules\n",
    "            x = inputs\n",
    "            # Enlarge the range of values in x to make max(new_x)=1 and others < 0\n",
    "            x = (x - K.max(x, 1, True)) / K.epsilon() + 1\n",
    "            mask = K.clip(x, 0, 1)  # the max value in x clipped to 1 and other to 0\n",
    "\n",
    "        # masked inputs, shape = [batch_size, dim_vector]\n",
    "        inputs_masked = K.batch_dot(inputs, mask, [1, 1])\n",
    "        return inputs_masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  # true label provided\n",
    "            return tuple([None, input_shape[0][-1]])\n",
    "        else:\n",
    "            return tuple([None, input_shape[-1]])\n",
    "\n",
    "\n",
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm)\n",
    "    return scale * vectors\n",
    "\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n",
    "    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_vector: dimension of the output vectors of the capsules in this layer\n",
    "    :param num_routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_vector = dim_vector\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_vector = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='W')\n",
    "\n",
    "        # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation.\n",
    "        self.bias = self.add_weight(shape=[1, self.input_num_capsule, self.num_capsule, 1, 1],\n",
    "                                    initializer=self.bias_initializer,\n",
    "                                    name='bias',\n",
    "                                    trainable=False)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_vector]\n",
    "        # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n",
    "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n",
    "\n",
    "        \"\"\"  \n",
    "        # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size.\n",
    "        # Now W has shape  = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector]\n",
    "        w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1])\n",
    "        \n",
    "        # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3])\n",
    "        \"\"\"\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow.\n",
    "        # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n",
    "                             elems=inputs_tiled,\n",
    "                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n",
    "        \"\"\"\n",
    "        # Routing algorithm V1. Use tf.while_loop in a dynamic way.\n",
    "        def body(i, b, outputs):\n",
    "            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "            b = b + K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "            return [i-1, b, outputs]\n",
    "\n",
    "        cond = lambda i, b, inputs_hat: i > 0\n",
    "        loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)]\n",
    "        _, _, outputs = tf.while_loop(cond, body, loop_vars)\n",
    "        \"\"\"\n",
    "        # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance\n",
    "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n",
    "            # outputs.shape=[None, 1, num_capsule, 1, dim_vector]\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "\n",
    "            # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n",
    "            if i != self.num_routing - 1:\n",
    "                # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True))\n",
    "                self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "            # tf.summary.histogram('BigBee', self.bias)  # for debugging\n",
    "        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_vector])\n",
    "\n",
    "\n",
    "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_vector: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_vector]\n",
    "    \"\"\"\n",
    "    output = layers.Conv2D(filters=dim_vector*n_channels, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_vector])(output)\n",
    "    return layers.Lambda(squash)(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7cd17730-22b6-4ac3-a612-31f18902fa78",
    "_uuid": "61c38c7ee701bb3ee2190263cf907fcdbe40dca2"
   },
   "source": [
    "# Build the Model\n",
    "Here we use the layers to build up the model. The model is a bit different from a standard $X\\rightarrow y$  model, it is $(X,y)\\rightarrow (y,X)$ meaning it attempts to predict the class from the image, and then at the same time, using the same capsule reconstruct the image from the class. The approach appears very cGAN-like where the task of reconstructing better helps the model 'understand' the image data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "bc101123-d53c-4c2e-a187-101c434885da",
    "_uuid": "2497453eb1895f624ad84617dd98c230f5640304",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "def CapsNet(input_shape, n_class, num_routing):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_shape: data shape, 4d, [None, width, height, channels]\n",
    "    :param n_class: number of classes\n",
    "    :param num_routing: number of routing iterations\n",
    "    :return: A Keras Model with 2 inputs and 2 outputs\n",
    "    \"\"\"\n",
    "    x = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Layer 1: Just a conventional Conv2D layer\n",
    "    conv1 = layers.Conv2D(filters=128, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "\n",
    "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_vector]\n",
    "    primarycaps = PrimaryCap(conv1, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "\n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_vector=16, num_routing=num_routing, name='digitcaps')(primarycaps)\n",
    "\n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out_caps = Length(name='out_caps')(digitcaps)\n",
    "\n",
    "    # Decoder network.\n",
    "    y = layers.Input(shape=(n_class,))\n",
    "    masked = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer.\n",
    "    x_recon = layers.Dense(128, activation='relu')(masked)\n",
    "    x_recon = layers.Dense(256, activation='relu')(x_recon)\n",
    "    x_recon = layers.Dense(IMG_SIZE**2, activation='sigmoid')(x_recon)\n",
    "    x_recon = layers.Reshape(target_shape=[IMG_SIZE, IMG_SIZE, 1], name='out_recon')(x_recon)\n",
    "\n",
    "    # two-input-two-output keras Model\n",
    "    return models.Model([x, y], [out_caps, x_recon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "c6d84e5a-c33c-40c8-89c3-aba3454f7025",
    "_uuid": "9f27c6b0623ebffb6c8a24579f9dd4e321d6b1c2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "6f168849-f8f9-4241-9ba8-59abc59573f1",
    "_uuid": "d21637e677fca5be415b2ff2e8a94ef7db2ea8a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 128, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 120, 120, 128 10496       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 56, 56, 256)  2654464     conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 100352, 8)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 100352, 8)    0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "digitcaps (CapsuleLayer)        (None, 2, 16)        25890816    lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_1 (Mask)                   (None, 16)           0           digitcaps[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          2176        mask_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          33024       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16384)        4210688     dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out_caps (Length)               (None, 2)            0           digitcaps[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "out_recon (Reshape)             (None, 128, 128, 1)  0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 32,801,664\n",
      "Trainable params: 32,600,960\n",
      "Non-trainable params: 200,704\n",
      "__________________________________________________________________________________________________\n",
      "No fancy plot Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = CapsNet(input_shape=[IMG_SIZE, IMG_SIZE, 1],\n",
    "                n_class=2,\n",
    "                num_routing=3)\n",
    "model.summary()\n",
    "try:\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "except Exception as e:\n",
    "    print('No fancy plot {}'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "e698ab37-4e74-43e6-b35b-a0507449d916",
    "_uuid": "c0374dabdf452026e3f231ffe689b5dd9f99288b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, data, epoch_size_frac=1.0):\n",
    "    \"\"\"\n",
    "    Training a CapsuleNet\n",
    "    :param model: the CapsuleNet model\n",
    "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
    "    :param args: arguments\n",
    "    :return: The trained model\n",
    "    \"\"\"\n",
    "    # unpacking the data\n",
    "    (x_train, y_train), (x_test, y_test) = data\n",
    "\n",
    "    # callbacks\n",
    "    log = callbacks.CSVLogger('log.csv')\n",
    "    checkpoint = callbacks.ModelCheckpoint('weights-{epoch:02d}.h5',\n",
    "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  loss_weights=[1., 0.0005],\n",
    "                  metrics={'out_caps': 'accuracy'})\n",
    "\n",
    "    \n",
    "    # Training without data augmentation:\n",
    "    history = model.fit([x_train, y_train], [y_train, x_train], batch_size=16, epochs=2, verbose = 2,\n",
    "              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, checkpoint, lr_decay, TQDMNotebookCallback()])\n",
    "    \n",
    "    \"\"\"\n",
    "    # -----------------------------------Begin: Training with data augmentation -----------------------------------#\n",
    "    def train_generator(x, y, batch_size, shift_fraction=0.):\n",
    "        train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,\n",
    "                                           height_shift_range=shift_fraction)  # shift up to 2 pixel for MNIST\n",
    "        generator = train_datagen.flow(x, y, batch_size=batch_size)\n",
    "        while 1:\n",
    "            x_batch, y_batch = generator.next()\n",
    "            yield ([x_batch, y_batch], [y_batch, x_batch])\n",
    "\n",
    "    # Training with data augmentation. If shift_fraction=0., also no augmentation.\n",
    "    model.fit_generator(generator=train_generator(x_train, y_train, 64, 0.1),\n",
    "                        steps_per_epoch=int(epoch_size_frac*y_train.shape[0] / 64),\n",
    "                        epochs=1,\n",
    "                        validation_data=[[x_test, y_test], [y_test, x_test]],\n",
    "                        callbacks=[log, checkpoint, lr_decay])\n",
    "    # -----------------------------------End: Training with data augmentation -----------------------------------#\n",
    "    \"\"\"\n",
    "    model.save_weights('trained_model.h5')\n",
    "    print('Trained model saved to \\'trained_model.h5\\'')\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "2fda2834-7c9e-4ed6-b322-0b9e86415201",
    "_uuid": "07bbb33aaa1c7ec875798359e300bbcdba374659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 60 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ffcb0041a545d99386cf53e96e19f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df607471dd149debb3da3d7c9992059"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\keras\\callbacks.py:408: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      " - 411s - loss: nan - out_caps_loss: nan - out_recon_loss: 0.0863 - out_caps_acc: 0.4984 - val_loss: nan - val_out_caps_loss: nan - val_out_recon_loss: 0.0863 - val_out_caps_acc: 0.5333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c79095e2bb44a9e963f852ee01faf9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 407s - loss: nan - out_caps_loss: nan - out_recon_loss: 0.0857 - out_caps_acc: 0.4986 - val_loss: nan - val_out_caps_loss: nan - val_out_recon_loss: 0.0856 - val_out_caps_acc: 0.5333\n",
      "\n",
      "Trained model saved to 'trained_model.h5'\n"
     ]
    }
   ],
   "source": [
    "mod, hists = train(model=model, data=((x_train, y_train), (x_test[:60], y_test[:60])), \n",
    "      epoch_size_frac = 0.5) # do 10% of an epoch (takes too long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x194ae38b0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x194ae4a78d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = hists\n",
    "\n",
    "acc = history.history['out_caps_acc']\n",
    "val_acc = history.history['val_out_caps_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.plot(epochs, acc, 'red', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'blue', label='Validation acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Training and validation loss')\n",
    "plt.plot(epochs, loss, 'red', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'blue', label='Validation loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "169b7f78-12c7-4fed-886e-60024fe59339",
    "_uuid": "02b7db879a533e7bfb3116522bebf3867b23498c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(np.sqrt(num))\n",
    "    height = int(np.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
    "            img[:, :, 0]\n",
    "    return image\n",
    "\n",
    "def test(model, data):\n",
    "    x_test, y_test = data\n",
    "    y_pred, x_recon = model.predict([x_test, y_test], batch_size=100)\n",
    "    print('-'*50)\n",
    "    print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "\n",
    "    img = combine_images(np.concatenate([x_test[:50],x_recon[:50]]))\n",
    "    image = img * 255\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\"real_and_recon.png\")\n",
    "    print()\n",
    "    print('Reconstructed images are saved to ./real_and_recon.png')\n",
    "    print('-'*50)\n",
    "    plt.imshow(plt.imread(\"real_and_recon.png\", ))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2706080d-2d50-4876-bd4d-c0f3c4ce87b5",
    "_uuid": "9afcde53f3cf3ba3eeeed1f083241874b4cd84e2"
   },
   "source": [
    "# Show the results on the hold-out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "56a6fe58-fe97-45c8-95da-223297842f79",
    "_uuid": "25ac7feb2d111b82e755169288ffd47ebc4d196a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test(model=model, data=(x_test[:100], y_test[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bf642928-3097-4b69-86b4-a2d1f02c1c38",
    "_uuid": "0c1bea7f648c238828af6ceb1ccd3f5fd6d1f075"
   },
   "source": [
    "# Apply Model to the Competition Data\n",
    "Here we apply the model to the compitition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e3777e2b-6aff-45e2-b1dd-bee2f0b8eadd",
    "_uuid": "532948f338ea8cc3868ec0511f8866583fa1ab84",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('../input/test.csv')\n",
    "data_test = data_test.values.reshape(-1, IMG_SIZE, IMG_SIZE, 1).astype('float32') / 255.\n",
    "y_pred, _ = model.predict([data_test, \n",
    "                           np.zeros((data_test.shape[0],2))], # empty values for the second vector \n",
    "                           batch_size = 32, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0d4794e0-f351-4ad9-beb9-7e76ff457e0a",
    "_uuid": "c00b9d785f9bd7bef37b4f96bc137d18c1219114",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('submission.csv', 'w') as out_file:\n",
    "    out_file.write('ImageId,Label\\n')\n",
    "    for img_id, guess_label in enumerate(np.argmax(y_pred,1),1):\n",
    "        out_file.write('%d,%d\\n' % (img_id, guess_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
